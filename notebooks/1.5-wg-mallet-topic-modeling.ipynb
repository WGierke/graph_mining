{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import logging\n",
    "\n",
    "from src.models.corpus import HackernewsCorpus\n",
    "from gensim import utils, models\n",
    "from src.data.load_data import get_hackernews_files, load_hackernews_dataframe\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one HackerNews DataFrame\n",
    "files = get_hackernews_files()\n",
    "df = load_hackernews_dataframe(files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-27 10:17:26,001 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-12-27 10:17:28,836 : INFO : adding document #10000 to Dictionary(26163 unique tokens: ['bitch', 'showcases', 'reversible', 'dataloader', 'geekwire']...)\n",
      "2017-12-27 10:17:31,794 : INFO : adding document #20000 to Dictionary(38044 unique tokens: ['bitch', 'showcases', 'reversible', 'dataloader', 'enduring']...)\n",
      "2017-12-27 10:17:34,878 : INFO : adding document #30000 to Dictionary(46157 unique tokens: ['tiobe', 'dataloader', 'theyre', 'symbasync', 'kennedy']...)\n",
      "2017-12-27 10:17:37,692 : INFO : adding document #40000 to Dictionary(52718 unique tokens: ['tiobe', 'dataloader', 'theyre', 'symbasync', 'kennedy']...)\n",
      "2017-12-27 10:17:41,083 : INFO : adding document #50000 to Dictionary(58865 unique tokens: ['tiobe', 'dataloader', 'theyre', 'symbasync', 'kennedy']...)\n",
      "2017-12-27 10:17:43,923 : INFO : adding document #60000 to Dictionary(64014 unique tokens: ['tiobe', 'dataloader', 'theyre', 'symbasync', 'ksfo']...)\n",
      "2017-12-27 10:17:47,092 : INFO : adding document #70000 to Dictionary(68496 unique tokens: ['tiobe', 'dataloader', 'theyre', 'symbasync', 'ksfo']...)\n",
      "2017-12-27 10:17:50,460 : INFO : adding document #80000 to Dictionary(72906 unique tokens: ['tiobe', 'dataloader', 'theyre', 'symbasync', 'ksfo']...)\n",
      "2017-12-27 10:17:53,336 : INFO : adding document #90000 to Dictionary(77419 unique tokens: ['tiobe', 'dataloader', 'theyre', 'symbasync', 'ksfo']...)\n",
      "2017-12-27 10:17:56,485 : INFO : adding document #100000 to Dictionary(81487 unique tokens: ['tiobe', 'dataloader', 'theyre', 'symbasync', 'ksfo']...)\n",
      "2017-12-27 10:17:59,198 : INFO : adding document #110000 to Dictionary(85430 unique tokens: ['tiobe', 'dataloader', 'theyre', 'symbasync', 'ksfo']...)\n",
      "2017-12-27 10:18:02,814 : INFO : adding document #120000 to Dictionary(88975 unique tokens: ['codingblocks', 'theyre', 'symbasync', 'making_ui_eas', 'favorable']...)\n",
      "2017-12-27 10:18:05,789 : INFO : adding document #130000 to Dictionary(92918 unique tokens: ['intreasting', 'codingblocks', 'theyre', 'symbasync', 'making_ui_eas']...)\n",
      "2017-12-27 10:18:09,405 : INFO : adding document #140000 to Dictionary(96423 unique tokens: ['intreasting', 'codingblocks', 'theyre', 'symbasync', 'making_ui_eas']...)\n",
      "2017-12-27 10:18:12,173 : INFO : adding document #150000 to Dictionary(99870 unique tokens: ['intreasting', 'codingblocks', 'theyre', 'symbasync', 'making_ui_eas']...)\n",
      "2017-12-27 10:18:15,032 : INFO : adding document #160000 to Dictionary(103027 unique tokens: ['intreasting', 'codingblocks', 'theyre', 'symbasync', 'making_ui_eas']...)\n",
      "2017-12-27 10:18:17,911 : INFO : adding document #170000 to Dictionary(106225 unique tokens: ['intreasting', 'codingblocks', 'theyre', 'symbasync', 'geoscience']...)\n",
      "2017-12-27 10:18:21,228 : INFO : adding document #180000 to Dictionary(109622 unique tokens: ['intreasting', 'codingblocks', 'theyre', 'symbasync', 'geoscience']...)\n",
      "2017-12-27 10:18:22,687 : INFO : built Dictionary(111129 unique tokens: ['intreasting', 'codingblocks', 'theyre', 'symbasync', 'geoscience']...) from 185125 documents (total 11287770 corpus positions)\n",
      "2017-12-27 10:18:22,940 : INFO : discarding 78098 tokens: [('of', 98214), ('the', 130555), ('is', 93051), ('noimplicitany', 2), ('to', 115985), ('and', 99082), ('femboner', 1), ('undressing', 1), ('babes', 1), ('rafe', 1)]...\n",
      "2017-12-27 10:18:22,942 : INFO : keeping 33031 tokens which were in no less than 5 and no more than 92562 (=50.0%) documents\n",
      "2017-12-27 10:18:23,037 : INFO : resulting dictionary: Dictionary(33031 unique tokens: ['bitch', 'tiobe', 'reversible', 'dataloader', 'theyre']...)\n"
     ]
    }
   ],
   "source": [
    "# Focus on the stories\n",
    "comments_df = df[df.type == \"comment\"]\n",
    "comments_df = comments_df.dropna(subset=['text'])\n",
    "comments = comments_df['text']\n",
    "\n",
    "# Set up the streamed corpus\n",
    "corpus = HackernewsCorpus(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-27 10:18:23,059 : INFO : serializing temporary corpus to /tmp/b13b75_corpus.txt\n",
      "2017-12-27 10:19:32,386 : INFO : converting temporary corpus to MALLET format with /home/madness/Programs/mallet-2.0.6/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /tmp/b13b75_corpus.txt --output /tmp/b13b75_corpus.mallet\n",
      "2017-12-27 10:19:46,572 : INFO : training MALLET LDA with /home/madness/Programs/mallet-2.0.6/bin/mallet train-topics --input /tmp/b13b75_corpus.mallet --num-topics 10  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state /tmp/b13b75_state.mallet.gz --output-doc-topics /tmp/b13b75_doctopics.txt --output-topic-keys /tmp/b13b75_topickeys.txt --num-iterations 1000 --inferencer-filename /tmp/b13b75_inferencer.mallet --doc-topics-threshold 0.0\n",
      "2017-12-27 10:32:39,372 : INFO : loading assigned topics from /tmp/b13b75_state.mallet.gz\n"
     ]
    }
   ],
   "source": [
    "# Train 10 LDA topics using MALLET\n",
    "mallet_path = '/home/madness/Programs/mallet-2.0.6/bin/mallet'\n",
    "model = models.wrappers.LdaMallet(mallet_path, corpus, num_topics=10, id2word=corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-27 10:33:28,550 : INFO : serializing temporary corpus to /tmp/b13b75_corpus.txt\n",
      "2017-12-27 10:33:28,564 : INFO : converting temporary corpus to MALLET format with /home/madness/Programs/mallet-2.0.6/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /tmp/b13b75_corpus.txt --output /tmp/b13b75_corpus.mallet.infer --use-pipe-from /tmp/b13b75_corpus.mallet\n",
      "2017-12-27 10:33:37,731 : INFO : inferring topics with MALLET LDA '/home/madness/Programs/mallet-2.0.6/bin/mallet infer-topics --input /tmp/b13b75_corpus.mallet.infer --inferencer /tmp/b13b75_inferencer.mallet --output-doc-topics /tmp/b13b75_doctopics.txt.infer --num-iterations 100 --doc-topics-threshold 0.0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 0.28205128205128205), (9, 0.11538461538461539), (0, 0.0868945868945869), (6, 0.08547008547008547), (5, 0.08262108262108261), (2, 0.07407407407407407), (7, 0.07122507122507123), (4, 0.06837606837606838), (3, 0.06837606837606838), (1, 0.06552706552706553)]\n"
     ]
    }
   ],
   "source": [
    "# Predict the topics of a document\n",
    "doc = \"As someone primarily interested in interpretation of deep models, I strongly resonate with this warning against anthropomorphization of neural networks. Deep learning isn't special; deep models tend to be more accurate than other methods, but fundamentally they aren't much closer to working like the human brain than e.g. gradient boosting models.\"\n",
    "bow = corpus.dictionary.doc2bow(utils.simple_preprocess(doc))\n",
    "print(model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t5\tdata don system service security access network internet run doesn server case account running information key services hardware set \n",
      "1\t5\tgoogle open web app source support windows user linux facebook users game apple version apps site content ve file \n",
      "2\t5\ttime ve don people years good things work day lot back ll find long thing didn feel life days \n",
      "3\t5\tcost high car low bitcoin power price space amount buy small costs big cars year large expensive higher long \n",
      "4\t5\thttps href rel nofollow www http org news github en wiki wikipedia amp html id ycombinator item article blog \n",
      "5\t5\tquot gt don people point article comment doesn person read wrong question bad thing isn understand talking women kind \n",
      "6\t5\twork money company companies business job market make pay time product software good working tech experience team full jobs \n",
      "7\t5\tpeople gt world government don public care law state social country free china society legal tax countries poor system \n",
      "8\t5\tproblem problems point human real things make important good model general research interesting learning change world isn ai science \n",
      "9\t5\tcode gt language pre type data write programming system design python languages js writing easy simple function performance level \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(model.ftopickeys()) as input:\n",
    "    topic_keys_lines = input.read()\n",
    "print(topic_keys_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3_graph",
   "language": "python",
   "name": "venv3_graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
